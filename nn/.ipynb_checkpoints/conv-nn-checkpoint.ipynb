{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.003\n",
    "training_epochs = 10\n",
    "display_step = 5\n",
    "\n",
    "# To prevent overfitting\n",
    "dropout = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-53965f6558f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mavailable_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m     \"\"\"\n\u001b[1;32m    813\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Dataset and iterator creation\n",
    "\n",
    "in_dir = \"../input/normalized-images/\"\n",
    "sample_size = 7000\n",
    "\n",
    "data = pd.read_csv('../input/train.csv')\n",
    "available_list = np.array([os.path.splitext(filename)[0] for filename in os.listdir(in_dir)])\n",
    "data = data[data[\"id\"].isin(available_list)]\n",
    "data = data.groupby('landmark_id', group_keys=False).apply(lambda df: df.sample(sample_size, random_state=123))\n",
    "full_url = np.vectorize(lambda x: in_dir+x+\".jpg\")\n",
    "filenames = full_url(data[\"id\"].values)\n",
    "labels = pd.get_dummies(data[\"landmark_id\"]).values\n",
    "train_filenames, test_filenames = filenames[:int(filenames.shape[0]*0.75)], filenames[int(filenames.shape[0]*0.75):]\n",
    "train_labels, test_labels = labels[:int(labels.shape[0]*0.75)], labels[int(labels.shape[0]*0.75):]\n",
    "\n",
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string)\n",
    "    image_decoded.set_shape((256, 256, 1))\n",
    "    return image_decoded, label\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_filenames, train_labels))\n",
    "    train_data = train_data.shuffle(buffer_size=10000)\n",
    "\n",
    "    # for a small batch size\n",
    "    train_data = train_data.map(_parse_function, num_parallel_calls=4)\n",
    "    train_data = train_data.batch(batch_size)\n",
    "\n",
    "    # for a large batch size (hundreds or thousands)\n",
    "    # dataset = dataset.apply(tf.contrib.data.map_and_batch(\n",
    "    #    map_func=_parse_function, batch_size=batch_size))\n",
    "\n",
    "    # with gpu usage\n",
    "    train_data = train_data.prefetch(1)\n",
    "\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_filenames, test_labels))\n",
    "    test_data = test_data.map(_parse_function, num_parallel_calls=4)\n",
    "    test_data = test_data.batch(batch_size)\n",
    "\n",
    "    iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                               train_data.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init = iterator.make_initializer(train_data) # Inicializador para train_data\n",
    "    test_init = iterator.make_initializer(test_data) # Inicializador para test_data\n",
    "\n",
    "# Total ammount of landmarks\n",
    "n_landmarks = len(data.groupby(\"landmark_id\")[\"landmark_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 256, 256, 1])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, n_landmarks])\n",
    "\n",
    "def conv2d(img, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add\\\n",
    "        (tf.nn.conv2d(img, w,\\\n",
    "        strides=[1, 1, 1, 1],\\\n",
    "        padding='SAME'),b))\n",
    "\n",
    "def max_pool(img, k):\n",
    "    return tf.nn.max_pool(img, \\\n",
    "        ksize=[1, k, k, 1],\\\n",
    "        strides=[1, k, k, 1],\\\n",
    "        padding='SAME')\n",
    "\n",
    "# weights and bias conv layer 1\n",
    "wc1 = tf.Variable(tf.random_normal([3, 3, 1, 32]))\n",
    "bc1 = tf.Variable(tf.random_normal([32]))\n",
    "\n",
    "# conv layer\n",
    "conv1 = conv2d(x,wc1,bc1)\n",
    "\n",
    "# Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 64*64 matrix.\n",
    "conv1 = max_pool(conv1, k=4)\n",
    "\n",
    "# dropout to reduce overfitting\n",
    "keep_prob = tf. placeholder(tf.float32)\n",
    "conv1 = tf.nn.dropout(conv1,keep_prob)\n",
    "\n",
    "# weights and bias conv layer 2\n",
    "wc2 = tf.Variable(tf.random_normal([3, 3, 32, 64]))\n",
    "bc2 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "# conv layer\n",
    "conv2 = conv2d(conv1,wc2,bc2)\n",
    "\n",
    "# Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 16*16 matrix.\n",
    "conv2 = max_pool(conv2, k=4)\n",
    "\n",
    "# dropout to reduce overfitting\n",
    "conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "\n",
    "'''\n",
    "# weights and bias conv layer 2\n",
    "wc3 = tf.Variable(tf.random_normal([1, 1, 64, 64]))\n",
    "bc3 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "# conv layer\n",
    "conv3 = conv2d(conv2,wc3,bc3)\n",
    "\n",
    "# Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 16*16 matrix.\n",
    "conv3 = max_pool(conv3, k=2)\n",
    "\n",
    "# dropout to reduce overfitting\n",
    "conv3 = tf.nn.dropout(conv3, keep_prob)\n",
    "'''\n",
    "\n",
    "# weights and bias fc 1\n",
    "wd1 = tf.Variable(tf.random_normal([16*16*64, 512]))\n",
    "bd1 = tf.Variable(tf.random_normal([512]))\n",
    "\n",
    "# fc 1\n",
    "dense1 = tf.reshape(conv2, [-1, wd1.get_shape().as_list()[0]])\n",
    "dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, wd1),bd1))\n",
    "dense1 = tf.nn.dropout(dense1, keep_prob)\n",
    "\n",
    "# weights and bias fc 2\n",
    "wd2 = tf.Variable(tf.random_normal([512, 512]))\n",
    "bd2 = tf.Variable(tf.random_normal([512]))\n",
    "\n",
    "# fc 2\n",
    "dense2 = tf.reshape(dense1, [-1, wd2.get_shape().as_list()[0]])\n",
    "dense2 = tf.nn.relu(tf.add(tf.matmul(dense2, wd2),bd2))\n",
    "dense2 = tf.nn.dropout(dense2, keep_prob)\n",
    "\n",
    "# weights and bias out\n",
    "wout = tf.Variable(tf.random_normal([512, n_landmarks]))\n",
    "bout = tf.Variable(tf.random_normal([n_landmarks]))\n",
    "\n",
    "# prediction\n",
    "pred = tf.add(tf.matmul(dense2, wout), bout)\n",
    "\n",
    "# softmax\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session start\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Required to get the filename matching to run.\n",
    "    sess.run(init)\n",
    "    \n",
    "    step = 1\n",
    "    # Compute epochs.\n",
    "    for i in range(training_epochs):\n",
    "        print(\"epoch: {}\".format(i))\n",
    "        sess.run(train_init)\n",
    "        try:\n",
    "            while True:\n",
    "                batch_xs, batch_ys = sess.run(next_element)\n",
    "                                \n",
    "                sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout}) \n",
    "                \n",
    "                if step % display_step == 0:\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys,  keep_prob: 1.})\n",
    "                    loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "                    print(\"step: {}\".format(step))\n",
    "                    print(\"accuracy: {}\".format(acc))\n",
    "                    print(\"loss: {}\".format(loss))\n",
    "                    print(\"\\n\")\n",
    "                step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "    # Test\n",
    "    print(\"Test\\n\")\n",
    "    sess.run(test_init)\n",
    "    try:\n",
    "        while True:\n",
    "            batch_xs, batch_ys = sess.run(next_element)\n",
    "\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys,  keep_prob: 1.})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print(\"accuracy: {}\".format(acc))\n",
    "            print(\"loss: {}\".format(loss))\n",
    "            print(\"\\n\")\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
