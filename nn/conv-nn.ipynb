{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.003\n",
    "training_epochs = 10\n",
    "display_step = 5\n",
    "\n",
    "# To prevent overfitting\n",
    "dropout = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and iterator creation\n",
    "\n",
    "in_dir = \"../input/normalized-images/\"\n",
    "sample_size = 7000\n",
    "\n",
    "data = pd.read_csv('../input/train.csv')\n",
    "available_list = np.array([os.path.splitext(filename)[0] for filename in os.listdir(in_dir)])\n",
    "data = data[data[\"id\"].isin(available_list)]\n",
    "data = data.groupby('landmark_id', group_keys=False).apply(lambda df: df.sample(sample_size, random_state=123))\n",
    "full_url = np.vectorize(lambda x: in_dir+x+\".jpg\")\n",
    "filenames = full_url(data[\"id\"].values)\n",
    "labels = pd.get_dummies(data[\"landmark_id\"]).values\n",
    "train_filenames, test_filenames = filenames[:int(filenames.shape[0]*0.75)], filenames[int(filenames.shape[0]*0.75):]\n",
    "train_labels, test_labels = labels[:int(labels.shape[0]*0.75)], labels[int(labels.shape[0]*0.75):]\n",
    "\n",
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string)\n",
    "    image_decoded.set_shape((256, 256, 1))\n",
    "    return image_decoded, label\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_filenames, train_labels))\n",
    "    train_data = train_data.shuffle(buffer_size=10000)\n",
    "\n",
    "    # for a small batch size\n",
    "    train_data = train_data.map(_parse_function, num_parallel_calls=4)\n",
    "    train_data = train_data.batch(batch_size)\n",
    "\n",
    "    # for a large batch size (hundreds or thousands)\n",
    "    # dataset = dataset.apply(tf.contrib.data.map_and_batch(\n",
    "    #    map_func=_parse_function, batch_size=batch_size))\n",
    "\n",
    "    # with gpu usage\n",
    "    train_data = train_data.prefetch(1)\n",
    "\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_filenames, test_labels))\n",
    "    test_data = test_data.map(_parse_function, num_parallel_calls=4)\n",
    "    test_data = test_data.batch(batch_size)\n",
    "\n",
    "    iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                               train_data.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init = iterator.make_initializer(train_data) # Inicializador para train_data\n",
    "    test_init = iterator.make_initializer(test_data) # Inicializador para test_data\n",
    "\n",
    "# Total ammount of landmarks\n",
    "n_landmarks = len(data.groupby(\"landmark_id\")[\"landmark_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 256, 256, 1])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, n_landmarks])\n",
    "\n",
    "def conv2d(img, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add\\\n",
    "        (tf.nn.conv2d(img, w,\\\n",
    "        strides=[1, 1, 1, 1],\\\n",
    "        padding='SAME'),b))\n",
    "\n",
    "def max_pool(img, k):\n",
    "    return tf.nn.max_pool(img, \\\n",
    "        ksize=[1, k, k, 1],\\\n",
    "        strides=[1, k, k, 1],\\\n",
    "        padding='SAME')\n",
    "\n",
    "# weights and bias conv layer 1\n",
    "wc1 = tf.Variable(tf.random_normal([5, 5, 1, 32]))\n",
    "bc1 = tf.Variable(tf.random_normal([32]))\n",
    "\n",
    "# conv layer\n",
    "conv1 = conv2d(x,wc1,bc1)\n",
    "\n",
    "# Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 128*128 matrix.\n",
    "conv1 = max_pool(conv1, k=2)\n",
    "\n",
    "# dropout to reduce overfitting\n",
    "keep_prob = tf. placeholder(tf.float32)\n",
    "conv1 = tf.nn.dropout(conv1,keep_prob)\n",
    "\n",
    "# weights and bias conv layer 2\n",
    "wc2 = tf.Variable(tf.random_normal([5, 5, 32, 64]))\n",
    "bc2 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "# conv layer\n",
    "conv2 = conv2d(conv1,wc2,bc2)\n",
    "\n",
    "# Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 64*64 matrix.\n",
    "conv2 = max_pool(conv2, k=2)\n",
    "\n",
    "# dropout to reduce overfitting\n",
    "conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "\n",
    "# weights and bias conv layer 2\n",
    "wc3 = tf.Variable(tf.random_normal([5, 5, 64, 64]))\n",
    "bc3 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "# conv layer\n",
    "conv3 = conv2d(conv2,wc3,bc3)\n",
    "\n",
    "# Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 32*32 matrix.\n",
    "conv3 = max_pool(conv3, k=2)\n",
    "\n",
    "# dropout to reduce overfitting\n",
    "conv3 = tf.nn.dropout(conv3, keep_prob)\n",
    "\n",
    "# weights and bias fc 1\n",
    "wd1 = tf.Variable(tf.random_normal([64*64*64, 256]))\n",
    "bd1 = tf.Variable(tf.random_normal([256]))\n",
    "\n",
    "# fc 1\n",
    "dense1 = tf.reshape(conv3, [-1, wd1.get_shape().as_list()[0]])\n",
    "dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, wd1),bd1))\n",
    "dense1 = tf.nn.dropout(dense1, keep_prob)\n",
    "\n",
    "# weights and bias fc 2\n",
    "wd2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "bd2 = tf.Variable(tf.random_normal([256]))\n",
    "\n",
    "# fc 2\n",
    "dense2 = tf.reshape(dense1, [-1, wd2.get_shape().as_list()[0]])\n",
    "dense2 = tf.nn.relu(tf.add(tf.matmul(dense2, wd2),bd2))\n",
    "dense2 = tf.nn.dropout(dense2, keep_prob)\n",
    "\n",
    "# weights and bias out\n",
    "wout = tf.Variable(tf.random_normal([256, n_landmarks]))\n",
    "bout = tf.Variable(tf.random_normal([n_landmarks]))\n",
    "\n",
    "# prediction\n",
    "pred = tf.add(tf.matmul(dense2, wout), bout)\n",
    "\n",
    "# softmax\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session start\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Required to get the filename matching to run.\n",
    "    sess.run(init)\n",
    "    \n",
    "    step = 1\n",
    "    # Compute epochs.\n",
    "    for i in range(training_epochs):\n",
    "        print(\"epoch: {}\".format(i))\n",
    "        sess.run(train_init)\n",
    "        try:\n",
    "            while True:\n",
    "                batch_xs, batch_ys = sess.run(next_element)\n",
    "                                \n",
    "                sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout}) \n",
    "                \n",
    "                if step % display_step == 0:\n",
    "                    acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys,  keep_prob: 1.})\n",
    "                    loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "                    print(\"step: {}\".format(step))\n",
    "                    print(\"accuracy: {}\".format(acc))\n",
    "                    print(\"loss: {}\".format(loss))\n",
    "                    print(\"\\n\")\n",
    "                step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "    # Test\n",
    "    print(\"Test\\n\")\n",
    "    sess.run(test_init)\n",
    "    try:\n",
    "        while True:\n",
    "            batch_xs, batch_ys = sess.run(next_element)\n",
    "\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys,  keep_prob: 1.})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print(\"accuracy: {}\".format(acc))\n",
    "            print(\"loss: {}\".format(loss))\n",
    "            print(\"\\n\")\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
